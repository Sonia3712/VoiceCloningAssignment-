{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec5fd8b-1745-49da-aa28-cb7f79dbf799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n",
      "Starting execution...\n",
      "Using PDF: C:\\Users\\Admin\\Downloads\\max-tegmark-life-30-being-human-in-the-age-of-artificial-intelligence-alfred-a-knopf-2017-aTvn.pdf\n",
      "Using reference audio: C:\\Users\\Admin\\Downloads\\myvoice.mp3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pickle\n",
    "import pdfplumber\n",
    "import torch\n",
    "from openvoice.api import BaseSpeakerTTS, ToneColorConverter\n",
    "from openvoice import se_extractor\n",
    "from pydub import AudioSegment\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "print(\"Starting execution...\")\n",
    "\n",
    "# Define paths\n",
    "downloads_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\n",
    "pdf_files = [f for f in os.listdir(downloads_dir) if f.endswith(\".pdf\")]\n",
    "audio_files = [f for f in os.listdir(downloads_dir) if f.endswith((\".mp3\", \".wav\"))]\n",
    "pdf_path = os.path.join(downloads_dir, pdf_files[0]) if pdf_files else None\n",
    "reference_audio = os.path.join(downloads_dir, audio_files[0]) if audio_files else None\n",
    "output_dir = r'D:\\chunks'  # Temporary chunks on D drive\n",
    "output_file = r'D:\\Life_3.0_audiobook.mp3'  # Final MP3 on D drive\n",
    "credentials_file = r'C:/Users/Admin/Desktop/client_secret_237595400842-smil8abge2j3lvvum3kf4963r64i6s46.apps.googleusercontent.com.json'\n",
    "drive_folder_id = \"1gh0Tk2YKxtPi8XTVR8AnH9pR_jedUF5t\"\n",
    "\n",
    "# Validate files\n",
    "if not pdf_path or not reference_audio:\n",
    "    raise ValueError(\"No PDF or audio file found in Downloads. Please add files (e.g., .pdf, .mp3, or .wav).\")\n",
    "\n",
    "print(f\"Using PDF: {pdf_path}\")\n",
    "print(f\"Using reference audio: {reference_audio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f59c5e89-563b-4d67-adce-007292c99bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking OpenVoice directory...\n",
      "Current directory: C:\\Users\\Admin\\Desktop\\zonos\\OpenVoice\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Check OpenVoice directory (clone if not present)\n",
    "print(\"Checking OpenVoice directory...\")\n",
    "if not os.path.exists(\"OpenVoice\"):\n",
    "    subprocess.run([\"git\", \"clone\", \"https://github.com/myshell-ai/OpenVoice\"], check=True)\n",
    "\n",
    "os.chdir('OpenVoice')\n",
    "print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5761fa59-8deb-4e28-89c8-61e9199cca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading checkpoint...\n",
      "Extracting checkpoint...\n"
     ]
    }
   ],
   "source": [
    "#Attempt to download checkpoint (optional: comment out if manually downloaded)\n",
    "checkpoint_url = 'https://myshell-public-repo-host.s3.amazonaws.com/openvoice/checkpoints_1226.zip'\n",
    "try:\n",
    "    print('Downloading checkpoint...')\n",
    "    subprocess.run(['powershell', '-Command', 'Invoke-WebRequest', '-Uri', checkpoint_url, '-OutFile', 'checkpoints_1226.zip'], check=True)\n",
    "    print('Extracting checkpoint...')\n",
    "    subprocess.run(['powershell', '-Command', 'Expand-Archive', '-Path', 'checkpoints_1226.zip', '-DestinationPath', '.'], check=True)\n",
    "    subprocess.run(['dir', 'checkpoints'], shell=True)\n",
    "except Exception as e:\n",
    "    print(f'Checkpoint download failed: {e}')\n",
    "    print('Using manually downloaded checkpoint (place checkpoints_1226.zip in OpenVoice and extract).')\n",
    "    # Ensure checkpoints folder exists manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17f7e48f-cfad-423c-94a3-5ad618845957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\utils\\weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint 'checkpoints/base_speakers/EN/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "Loaded checkpoint 'checkpoints/converter/checkpoint.pth'\n",
      "missing/unexpected keys: [] []\n",
      "OpenVoice model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load OpenVoice model\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "base_speaker_tts = BaseSpeakerTTS('checkpoints/base_speakers/EN/config.json', device=device)\n",
    "base_speaker_tts.load_ckpt('checkpoints/base_speakers/EN/checkpoint.pth')\n",
    "tone_color_converter = ToneColorConverter('checkpoints/converter/config.json', device=device)\n",
    "tone_color_converter.load_ckpt('checkpoints/converter/checkpoint.pth')\n",
    "print('OpenVoice model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8c2de97-bd13-4131-9d89-ad0d24eea118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 734945 characters from PDF\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Extract text from PDF\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    text = ''.join(page.extract_text() or '' for page in pdf.pages)\n",
    "print(f\"Extracted {len(text)} characters from PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84a5f15c-171c-4085-9c52-a5f9e81bce67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 387 chunks\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Chunk the text\n",
    "def chunk_text(text, max_length=1902):  # Adjusted to target ~368 chunks (e.g., for 700,000 characters)\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "text_chunks = chunk_text(text)\n",
    "print(f\"Created {len(text_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80c02d-3e8f-47c7-9d68-e73017c90c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenVoice version: v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/snakers4/silero-vad/zipball/master\" to C:\\Users\\Admin/.cache\\torch\\hub\\master.zip\n",
      "[(1.134, 16.274)]\n",
      "after vad: dur = 15.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\functional.py:730: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\SpectralOps.cpp:880.)\n",
      "  return _VF.stft(  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "Also by Max Tegmark Our Mathematical UniverseThis Is a Borzoi Book Published by Alfred A.\n",
      "Knopf Copyright Â© 2017 by Max Tegmark All rights reserved. Published in the United States by Alfred A.\n",
      "Knopf, a division of Penguin Random House LLC, New York, and distributed in Canada by Random House of Canada,\n",
      "a division of Penguin Random House Canada Limited, Toronto. www. aaknopf.\n",
      "com Knopf, Borzoi Books and the colophon are registered trademarks of Penguin Random House LLC.\n",
      "Library of Congress Cataloging-in-Publication Data Names: Tegmark, Max, author. Title: Life 3.\n",
      "0 : being human in the age of artificial intelligence / by Max Tegmark.\n",
      "Other titles: Life three point zero Description: New York : Alfred A.\n",
      "Knopf, 2017. | This is a Borzoi Book published by Alfred A.\n",
      "Knopf. | Includes bibliographical references and index. Identifiers: LCCN 2017006248 print | LCCN 2017022912 ebook | ISBN 9781101946596 hardcover | ISBN 9781101946602 ebook Subjects: LCSH: Artificial intelligenceâPhilosophy.\n",
      "| Artificial intelligenceâSocial aspects. | Automation âSocial aspects. | Artificial intelligenceâMoral and ethical aspects.\n",
      "| AutomationâMoral and ethical aspects. | Artificial intelligenceâPhilosophy. | Technological forecasting.\n",
      "| BISAC: TECHNOLOGY & ENGINEERING / Robotics. | SCIENCE / Experiments & Projects.\n",
      "| TECHNOLOGY & ENGINEERING / Inventions. Classification: LCC Q334. 7 ebook | LCC Q334.\n",
      "7 . T44 2017 print | DDC 006. 301âdc23 LC record available at https://lccn.\n",
      "loc. gov/2017006248 Ebook ISBN 9781101946602 Cover art by Suvadip Das; man based on Netfalls Remy Musser/Shutterstock Cover design by John Vorhees v4.\n",
      "1 epContents Cover Also by Max Tegmark Title Page Copyright Dedication Acknowledgments Prelude: The Tale of the Omega Team 1 Welcome to the Most Important Conversation of Our Time A Brief History of Complexity The Three Stages of Life Controversies Misconceptions The Road\n",
      " > ===========================\n",
      "ËÉlsoÊ baÉª mÃ¦ks tegmaÉ¹k* ÉÉ¹ ËmÃ¦Î¸ÉËmÃ¦tÉªkÉÉ« ËjunÉËvÉÉ¹s Ã°Éªs Éªz É boÉ¹zoi* bÊk ËpÉblÉªÊt baÉª ËÃ¦lfÉ¹Éªd É.\n",
      " length:97\n",
      " length:97\n",
      "nÉpf ËkÉpiËÉ¹aÉªt (si) tËwÉnti ËsÉvÉnËtin baÉª mÃ¦ks tegmaÉ¹k* ÉÉ« É¹aÉªts É¹ÉªËzÉÉ¹vd. ËpÉblÉªÊt Éªn Ã°É juËnaÉªtÉªd steÉªts baÉª ËÃ¦lfÉ¹Éªd É.\n",
      " length:123\n",
      " length:121\n",
      "nÉpf, É dÉªËvÉªÊÉn Év ËpÉÅgwÉn ËÉ¹Ã¦ndÉm haÊs É«lc*, nu jÉÉ¹k, Énd dÉªËstÉ¹ÉªbjÉtÉd Éªn ËkÃ¦nÉdÉ baÉª ËÉ¹Ã¦ndÉm haÊs Év ËkÃ¦nÉdÉ,\n",
      " length:114\n",
      " length:113\n",
      "É dÉªËvÉªÊÉn Év ËpÉÅgwÉn ËÉ¹Ã¦ndÉm haÊs ËkÃ¦nÉdÉ ËlÉªmÉªtÉªd, tÉÉ¹ËÉntoÊ. www*. aaknopf*.\n",
      " length:80\n",
      " length:80\n",
      "kÉm nÉpf, boÉ¹zoi* bÊks Énd Ã°É colophon* ÉÉ¹ ËÉ¹ÉdÊÉªstÉÉ¹d ËtÉ¹eÉªdËmÉÉ¹ks Év ËpÉÅgwÉn ËÉ¹Ã¦ndÉm haÊs É«lc*.\n",
      " length:98\n",
      " length:96\n",
      "ËlaÉªbÉ¹ÉËÉ¹i Év ËkÉÅgÉ¹És cataloging-in-publication* ËdÃ¦tÉ neÉªmz: tegmaÉ¹k*, mÃ¦ks, ËÉÎ¸ÉÉ¹. ËtaÉªtÉÉ«: laÉªf Î¸É¹i.\n",
      " length:104\n",
      " length:100\n",
      "ËziÉ¹oÊ biÉªÅ ËjumÉn Éªn Ã°É eÉªdÊ Év ËÉÉ¹tÉËfÉªÊÉÉ« ËÉªnËtÉlÉdÊÉns baÉª mÃ¦ks tegmaÉ¹k*.\n",
      " length:77\n",
      " length:77\n",
      "ËÉÃ°ÉÉ¹ ËtaÉªtÉÉ«z: laÉªf Î¸É¹i pÉÉªnt ËziÉ¹oÊ dÉªËskÉ¹ÉªpÊÉn: nu jÉÉ¹k ËÃ¦lfÉ¹Éªd É.\n",
      " length:69\n",
      " length:67\n",
      "nÉpf, tËwÉnti ËsÉvÉnËtin. Ã°Éªs Éªz É boÉ¹zoi* bÊk ËpÉblÉªÊt baÉª ËÃ¦lfÉ¹Éªd É.\n",
      " length:70\n",
      " length:70\n",
      "nÉpf. ËÉªnËkludz bibliogÉ¹aphicaÉ«* ËÉ¹ÉfÉÉ¹ÉnsÉªz Énd ËÉªndÉks. identifieÉ¹s*: É«ccn* tu ËbÉªljÉn, ËsÉvÉnËtin ËmÉªljÉn, sÉªks ËÎ¸aÊzÉnd, tu ËhÉnÉÉ¹d foÉ¹ty-eight* pÉ¹Éªnt É«ccn* tu ËbÉªljÉn, ËsÉvÉnËtin ËmÉªljÉn, tËwÉntiËtu ËÎ¸aÊzÉnd, naÉªn ËhÉnÉÉ¹d twÉÉ«v ebook* isbn* naÉªn ËtÉ¹ÉªljÉn, ËsÉvÉn ËhÉnÉÉ¹d eighty-one* ËbÉªljÉn, wÉn ËhÉnÉÉ¹d wÉn ËmÉªljÉn, naÉªn ËhÉnÉÉ¹d foÉ¹ty-six* ËÎ¸aÊzÉnd, faÉªv ËhÉnÉÉ¹d ninety-six* ËhÉÉ¹dËkÉvÉÉ¹ isbn* naÉªn ËtÉ¹ÉªljÉn, ËsÉvÉn ËhÉnÉÉ¹d eighty-one* ËbÉªljÉn, wÉn ËhÉnÉÉ¹d wÉn ËmÉªljÉn, naÉªn ËhÉnÉÉ¹d foÉ¹ty-six* ËÎ¸aÊzÉnd, sÉªks ËhÉnÉÉ¹d tu ebook* ËsÉbdÊÉªkts: É«csh*: ËÉÉ¹tÉËfÉªÊÉÉ« intelligence--philosophy*.\n",
      " length:589\n",
      " length:579\n",
      " ËÉÉ¹tÉËfÉªÊÉÉ« intelligence--sociaÉ«* ËÃ¦ËspÉkts. ÉtÉËmeÉªÊÉn --ËsoÊÊÉÉ« ËÃ¦ËspÉkts. ËÉÉ¹tÉËfÉªÊÉÉ« intelligence--moÉ¹aÉ«* Énd ËÉÎ¸ÉªkÉÉ« ËÃ¦ËspÉkts.\n",
      " length:133\n",
      " length:130\n",
      " automation--moÉ¹aÉ«* Énd ËÉÎ¸ÉªkÉÉ« ËÃ¦ËspÉkts. ËÉÉ¹tÉËfÉªÊÉÉ« intelligence--philosophy*. ËtÉknÉËlÉdÊÉªkÉÉ« ËfÉÉ¹ËkÃ¦stÉªÅ.\n",
      " length:110\n",
      " length:109\n",
      " bisac*: tÉkËnÉlÉdÊi ËÉndÊÉËnÉªÉ¹ÉªÅ ËÉ¹oÊËbÉtÉªks. saÉªÉns ÉªkËspÉÉ¹ÉmÉnts ËpÉ¹ÉdÊÉkts.\n",
      " length:79\n",
      " length:77\n",
      " tÉkËnÉlÉdÊi ËÉndÊÉËnÉªÉ¹ÉªÅ ËÉªnËvÉnÊÉnz. ËklÃ¦sÉfÉËkeÉªÊÉn: É«cc* qthÉ¹ee* ËhÉnÉÉ¹d thiÉ¹ty-fouÉ¹*. ËsÉvÉn ebook* É«cc* qthÉ¹ee* ËhÉnÉÉ¹d thiÉ¹ty-fouÉ¹*.\n",
      " length:139\n",
      " length:132\n",
      "ËsÉvÉn tfoÉ¹ty-fouÉ¹* tËwÉnti ËsÉvÉnËtin pÉ¹Éªnt ddc* sÉªks. Î¸É¹i ËhÉnÉÉ¹d one--dctwenty-thÉ¹ee* É«c* ËÉ¹ÉkÉÉ¹d ÉËveÉªlÉbÉÉ« Ã¦t https://É«ccn*.\n",
      " length:129\n",
      " length:121\n",
      "loc*. gov/two* ËbÉªljÉn, ËsÉvÉnËtin ËmÉªljÉn, sÉªks ËÎ¸aÊzÉnd, tu ËhÉnÉÉ¹d foÉ¹ty-eight* ebook* isbn* naÉªn ËtÉ¹ÉªljÉn, ËsÉvÉn ËhÉnÉÉ¹d eighty-one* ËbÉªljÉn, wÉn ËhÉnÉÉ¹d wÉn ËmÉªljÉn, naÉªn ËhÉnÉÉ¹d foÉ¹ty-six* ËÎ¸aÊzÉnd, sÉªks ËhÉnÉÉ¹d tu ËkÉvÉÉ¹ ÉÉ¹t baÉª suvadip* dÉs; mÃ¦n beÉªst Én netfaÉ«ls* ËÉ¹Émi musseÉ¹/shutteÉ¹stock* ËkÉvÉÉ¹ dÉªËzaÉªn baÉª dÊÉn ËvÉÉ¹hiz vfouÉ¹*.\n",
      " length:340\n",
      " length:335\n",
      "wÉn ep* ËkÉntÉnts ËkÉvÉÉ¹ ËÉlsoÊ baÉª mÃ¦ks tegmaÉ¹k* ËtaÉªtÉÉ« peÉªdÊ ËkÉpiËÉ¹aÉªt ËdÉdÉËkeÉªÊÉn acknowledgments* ËpÉ¹eÉªËlud: Ã°É teÉªÉ« Év Ã°É oÊËmÉgÉ tim wÉn ËwÉlkÉm tÉª Ã°É moÊst ËÉªmËpÉÉ¹tÉnt ËkÉnvÉÉ¹ËseÉªÊÉn Év ÉÉ¹ taÉªm É bÉ¹if ËhÉªstÉÉ¹i Év kÉmËplÉksÉªti Ã°É Î¸É¹i ËsteÉªdÊÉªz Év laÉªf ËkÉntÉ¹ÉËvÉÉ¹siz mÉªskÉnËsÉpÊÉnz Ã°É É¹oÊd.\n",
      " length:299\n",
      " length:297\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Generate cloned speech for each chunk\n",
    "if os.path.exists('processed'):\n",
    "    subprocess.run(['rmdir', '/S', '/Q', 'processed'], shell=True)\n",
    "\n",
    "# Extract source and target speaker embeddings\n",
    "source_se = torch.load('checkpoints/base_speakers/EN/en_default_se.pth', map_location=device)\n",
    "target_se, _ = se_extractor.get_se(reference_audio, tone_color_converter, target_dir='processed', vad=True)\n",
    "\n",
    "# Generate audio for each chunk\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    src_path = f'{output_dir}\\\\tmp_{i}.wav'\n",
    "    base_speaker_tts.tts(chunk, src_path, speaker='default', language='English', speed=1.0)\n",
    "    save_path = f'{output_dir}\\\\part_{i}.wav'\n",
    "    tone_color_converter.convert(\n",
    "        audio_src_path=src_path,\n",
    "        src_se=source_se,\n",
    "        tgt_se=target_se,\n",
    "        output_path=save_path,\n",
    "        message='@MyShell'\n",
    "    )\n",
    "    print(f'Generated audio for chunk {i+1}/{len(text_chunks)}: {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9411d5c6-9189-46e3-95db-dd57211cffaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\chunks\\\\part_0.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m combined \u001b[38;5;241m=\u001b[39m AudioSegment\u001b[38;5;241m.\u001b[39mempty()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(text_chunks)):\n\u001b[1;32m----> 4\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mAudioSegment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_wav\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mpart_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     combined \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m audio\n\u001b[0;32m      7\u001b[0m combined\u001b[38;5;241m.\u001b[39mexport(output_file, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydub\\audio_segment.py:808\u001b[0m, in \u001b[0;36mAudioSegment.from_wav\u001b[1;34m(cls, file, parameters)\u001b[0m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_wav\u001b[39m(\u001b[38;5;28mcls\u001b[39m, file, parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydub\\audio_segment.py:651\u001b[0m, in \u001b[0;36mAudioSegment.from_file\u001b[1;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m file, close_file \u001b[38;5;241m=\u001b[39m \u001b[43m_fd_or_path_or_tempfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtempfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m:\n\u001b[0;32m    654\u001b[0m     \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mlower()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pydub\\utils.py:60\u001b[0m, in \u001b[0;36m_fd_or_path_or_tempfile\u001b[1;34m(fd, mode, tempfile)\u001b[0m\n\u001b[0;32m     57\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fd, basestring):\n\u001b[1;32m---> 60\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     close_fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\chunks\\\\part_0.wav'"
     ]
    }
   ],
   "source": [
    "# Step 7: Merge audio chunks and save to D drive\n",
    "combined = AudioSegment.empty()\n",
    "for i in range(len(text_chunks)):\n",
    "    audio = AudioSegment.from_wav(f'{output_dir}\\\\part_{i}.wav')\n",
    "    combined += audio\n",
    "\n",
    "combined.export(output_file, format='mp3')\n",
    "print(f'Audiobook generated and saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ef6a0-becc-4343-a39b-10829c45d5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Upload to Google Drive\n",
    "print(\"Authenticating with Google Drive...\")\n",
    "flow = InstalledAppFlow.from_client_secrets_file(\n",
    "    credentials_file,\n",
    "    scopes=['https://www.googleapis.com/auth/drive.file']\n",
    ")\n",
    "creds = flow.run_local_server(port=0)\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "file_metadata = {\n",
    "    'name': 'Life_3.0_audiobook.mp3',\n",
    "    'parents': [drive_folder_id]\n",
    "}\n",
    "media = MediaFileUpload(output_file, mimetype='audio/mpeg')\n",
    "file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "print(f'File uploaded to Google Drive with ID: {file.get(\"id\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823b2ad-27f6-4624-89ba-259d684a949d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
